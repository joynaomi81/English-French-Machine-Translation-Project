# -*- coding: utf-8 -*-
"""Machine Translation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UnQETckS-Qzo-fuhOYZCdeveZ0k0v5mp
"""

#Import Libraries
import pandas as pd
import numpy as np
import re
import torch
from sklearn.model_selection import train_test_split
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset

!pip install transformers datasets sacrebleu sentencepiece -q

# load dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/damo_mt_testsets_en2fr_news_wmt13.csv")
df.head()

df.shape

df.info()

# Rename columns
df = df.rename(columns={'0': 'english', '1': 'french'})

df.head()

#Check for missing values
df.isnull().sum()

df.duplicated().sum()

"""Text Preprocessing"""

def clean_text(text):
    text = re.sub(r'<[^>]+>', '', text)     # remove HTML tags
    text = re.sub(r'\s+', ' ', text)        # remove extra spaces
    text = text.strip()                     # trim spaces
    return text

df['english'] = df['english'].apply(clean_text)
df['french'] = df['french'].apply(clean_text)
df.head()

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

df['src_len'] = df['english'].apply(lambda x: len(str(x).split()))
df['tgt_len'] = df['french'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(10, 5))
sns.histplot(df['src_len'], bins=30, color='blue', kde=True, label='Source (EN)')
sns.histplot(df['tgt_len'], bins=30, color='orange', kde=True, label='Target (FR)')
plt.xlabel("Sentence Length (words)")
plt.ylabel("Frequency")
plt.title("Sentence Length Distribution")
plt.legend()
plt.show()

#Boxplot for sentence length spread
plt.figure(figsize=(8, 5))
sns.boxplot(data=df[['src_len', 'tgt_len']], palette=['blue', 'orange'])
plt.xticks([0, 1], ['English', 'French'])
plt.ylabel("Sentence Length (words)")
plt.title("Sentence Length Spread")
plt.show()

# Check for unique values
df.nunique()

sources = df['english'].tolist()      # English sentences
references_str = df['french'].tolist()  # French reference sentences

from transformers import MBart50TokenizerFast, MBartForConditionalGeneration

model_name = "facebook/mbart-large-50-many-to-many-mmt"

tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

tokenizer.src_lang = "en_XX"  # Set source language to English

!pip install tqdm

sources_3003 = sources[:3003]

from tqdm import tqdm
batch_size = 16
predictions = []

for i in tqdm(range(0, len(sources_3003), batch_size)):
    batch_sentences = sources_3003[i:i+batch_size]

    # Tokenize the batch
    inputs = tokenizer(batch_sentences, return_tensors="pt", padding=True, truncation=True)

    # Generate translations
    generated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["fr_XX"],
        max_length=100
    )

    # Decode and add to predictions list
    batch_translations = [tokenizer.decode(t, skip_special_tokens=True) for t in generated_tokens]
    predictions.extend(batch_translations)

!pip install evaluate

from evaluate import load # Corrected import

import evaluate

!pip install rouge_score

import evaluate

# Slice the references to match sources_3003
references_3003_raw = references_str[:3003]       # list of strings
references_3003 = [[ref] for ref in references_3003_raw]  # list of lists for BLEU

# BLEU
bleu = evaluate.load("bleu")
bleu_results = bleu.compute(predictions=predictions, references=references_3003)
print("BLEU Score:", bleu_results["bleu"])

# ROUGE
rouge = evaluate.load("rouge")
rouge_results = rouge.compute(predictions=predictions, references=references_3003_raw)
print("ROUGE:", rouge_results)

# METEOR
meteor = evaluate.load("meteor")
meteor_results = meteor.compute(predictions=predictions, references=references_3003_raw)
print("METEOR Score:", meteor_results["meteor"])

import pandas as pd

# Create a DataFrame for inspection
sample_df = pd.DataFrame({
    "English Source": sources[:10],               # first 10 English sentences
    "Model Prediction": predictions[:10],         # first 10 predictions
    "Reference Translation": references_str[:10]  # first 10 reference translations
})

# Display
sample_df